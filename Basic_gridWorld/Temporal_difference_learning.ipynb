{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given policy evaluate value by Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphics import *\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create 5x5 Grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = GraphWin(\"My Window\", 600,600)\n",
    "win.setBackground(color_rgb(255,255,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(x1,y1,x2,y2):\n",
    "    \"\"\"\n",
    "    create a line between pt(x1,y1) and pt(x2,y2) and return it\n",
    "    \"\"\"\n",
    "    ln = Line(Point(x1,y1),Point(x2,y2))\n",
    "    ln.setOutline(color_rgb(0,0,0))\n",
    "    ln.setWidth(2)\n",
    "    \n",
    "    return ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rectangle(Point(150.0, 250.0), Point(250.0, 350.0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#border lines\n",
    "border1 = line(50,50,550,50)\n",
    "border1.draw(win)\n",
    "\n",
    "border2 = line(550,50,550,550)\n",
    "border2.draw(win)\n",
    "\n",
    "border3 = line(50,550,550,550)\n",
    "border3.draw(win)\n",
    "\n",
    "border4 = line(50,50,50,550)\n",
    "border4.draw(win)\n",
    "\n",
    "#lines horizontal\n",
    "ln1 = line(50,150,550,150)\n",
    "ln1.draw(win)\n",
    "\n",
    "ln2 = line(50,250,550,250)\n",
    "ln2.draw(win)\n",
    "\n",
    "ln3 = line(50,350,550,350)\n",
    "ln3.draw(win)\n",
    "\n",
    "ln4 = line(50,450,550,450)\n",
    "ln4.draw(win)\n",
    "\n",
    "#lines vertical\n",
    "\n",
    "lnv1 = line(150,50,150,550)\n",
    "lnv1.draw(win)\n",
    "\n",
    "lnv2 = line(250,50,250,550)\n",
    "lnv2.draw(win)\n",
    "\n",
    "lnv3 = line(350,50,350,550)\n",
    "lnv3.draw(win)\n",
    "\n",
    "lnv4 = line(450,50,450,550)\n",
    "lnv4.draw(win)\n",
    "\n",
    "#create goal, obstacle and hole\n",
    "\n",
    "rect_goal = Rectangle(Point(450,50), Point(550,150))\n",
    "rect_goal.setFill(color_rgb(0,255,0))\n",
    "rect_goal.draw(win)\n",
    "\n",
    "rect_hole = Rectangle(Point(450,150), Point(550,250))\n",
    "rect_hole.setFill(color_rgb(255,0,0))\n",
    "rect_hole.draw(win)\n",
    "\n",
    "rect_obs1 = Rectangle(Point(150,350), Point(250,450))\n",
    "rect_obs1.setFill(color_rgb(0,0,0))\n",
    "rect_obs1.draw(win)\n",
    "\n",
    "rect_obs2 = Rectangle(Point(150,250), Point(250,350))\n",
    "rect_obs2.setFill(color_rgb(0,0,0))\n",
    "rect_obs2.draw(win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create agent look like circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Circle(Point(100.0, 500.0), 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Circle(Point(100,500), 25)\n",
    "agent.setFill(color_rgb(0,0,255))\n",
    "agent.draw(win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if (val.getY()) == 100 or (val.getY() == 500 and val.getX() == 200) :\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(0,-100)\n",
    "        \n",
    "    \n",
    "def down(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getY() == 500 or (val.getX() == 200 and val.getY() == 200):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(0,100)\n",
    "\n",
    "def right(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getX() == 500 or (val.getX() == 100 and (val.getY() == 300 or val.getY() == 400)):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(100,0)\n",
    "\n",
    "def left(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getX() == 100 or (val.getX() == 300 or (val.getY() == 300 or val.getY() == 400)):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(-100,0)\n",
    "        \n",
    "def action(agent,val):\n",
    "    if val == 0:\n",
    "        up(agent)\n",
    "    elif val == 1:\n",
    "        down(agent)\n",
    "    elif val == 2:\n",
    "        right(agent)\n",
    "    else :\n",
    "        left(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward for agent in the each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.array([[-1,-1,-1,-1,-1],\n",
    "                   [-1,-20,-1,-1,-1],\n",
    "                   [-1,-20,-1,-1,-1],\n",
    "                   [-1,-1,-1,-1,-100],\n",
    "                   [-1,-1,-1,-1,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards(agent):\n",
    "    val = agent.getCenter()\n",
    "    i = int(val.getX() / 100) - 1\n",
    "    j = int((500 - val.getY()) / 100) \n",
    "    \n",
    "    \n",
    "    return reward[j][i]\n",
    "\n",
    "def observation(agent):\n",
    "    val = agent.getCenter()\n",
    "    i = int(val.getX() / 100) - 1\n",
    "    j = int((500 - val.getY()) / 100) \n",
    "    \n",
    "    return [i,j]\n",
    "\n",
    "def env_close():\n",
    "    win.close()\n",
    "\n",
    "def reset(agent):\n",
    "    agent.move(-400,400)\n",
    "\n",
    "def set_agent(agent,obs):\n",
    "    pos = agent.getCenter()\n",
    "    \n",
    "    pos = [pos.getX(), pos.getY()]\n",
    "    \n",
    "    refer_pos = [(obs[0]+1) * 100, (500 - (obs[1]*100))]\n",
    "    \n",
    "    \n",
    "    agent.move(refer_pos[0]-pos[0], refer_pos[1] - pos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the value function and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.array([[.0,.0,.0,.0,.0],\n",
    "                  [.0,.0,.0,.0,.0],\n",
    "                  [.0,.0,.0,.0,.0],\n",
    "                  [.0,.0,.0,.0,0.0],\n",
    "                  [.0,.0,.0,.0,100.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.array([[0,0,0,0,2],\n",
    "                   [2,0,0,0,2],\n",
    "                   [0,0,0,0,2],\n",
    "                   [0,0,0,0,2],\n",
    "                   [0,0,3,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 100\n",
    "max_episode_length = 100\n",
    "learning_rate = .1\n",
    "discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(episode):  \n",
    "    \n",
    "    for i in range(max_episode_length):\n",
    "            \n",
    "        state = observation(agent)\n",
    "        Reward = rewards(agent)\n",
    "        \n",
    "        if state[0]== 4 and state[1] == 4:\n",
    "            reset(agent)\n",
    "            continue\n",
    "        \n",
    "        act = policy[state[0],state[1]]\n",
    "        \n",
    "        #add probability to each action\n",
    "        if act == 0:\n",
    "            act = np.random.choice([0,1,2,3],p=[0.8,0,0.1,0.1])\n",
    "        elif act == 1:\n",
    "            act = np.random.choice([0,1,2,3],p=[0,0.8,0.1,0.1])\n",
    "        elif act == 2:\n",
    "            act = np.random.choice([0,1,2,3],p=[0.1,0.1,0.8,0])\n",
    "        elif act == 3:\n",
    "            act = np.random.choice([0,1,2,3],p=[0.1,0.1,0, 0.8])\n",
    "            \n",
    "        #act on the grid world\n",
    "        action(agent,act)\n",
    "        \n",
    "        new_state = observation(agent)\n",
    "        #temporal difference learning\n",
    "    \n",
    "        value[state[0],state[1]] = value[state[0],state[1]] + (learning_rate * (Reward + (discount_factor * value[new_state[0],new_state[1]]) - value[state[0],state[1]]))\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 27.54132439,  31.97147639,  37.1432433 ,  43.45061572,\n",
       "         48.91313252],\n",
       "       [ 31.59184643,   0.        ,   0.        ,  51.00021749,\n",
       "         58.50532916],\n",
       "       [ 37.18967125,  46.55691957,  54.54620664,  58.26468253,\n",
       "         70.93658646],\n",
       "       [ 23.94574182,  34.63002838,  49.17733588,  54.51486483,\n",
       "         86.05053942],\n",
       "       [ -5.25061197, -14.06459609, -14.6904507 , -17.86681492,\n",
       "        100.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value converge after 200 episode and 100 step per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 49 | 58 | 69 | 82 | 100 |\n",
    "|----|----|----|----|-----|\n",
    "| 42 | 46 | 57 | 69 | -9  |\n",
    "|----|----|----|----|-----|\n",
    "| 35 |  0 | 28 | 9  |-1.4 |\n",
    "|----|----|----|----|-----|\n",
    "| 29 |  0 | 9  | .3 |-.3  |\n",
    "|----|----|----|----|-----|\n",
    "| 22 |-.8 | 2  |-.2 |-.02 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value converge after 500 episode and 100 step per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 49 | 58 | 71 | 86 | 100 |\n",
    "|----|----|----|----|-----|\n",
    "| 43 | 51 | 58 | 54 | -17 |\n",
    "|----|----|----|----|-----|\n",
    "| 37 |  0 | 54 | 49 |-14  |\n",
    "|----|----|----|----|-----|\n",
    "| 32 |  0 | 46 | 34 |-14  |\n",
    "|----|----|----|----|-----|\n",
    "| 27 | 31 | 37 | 23 | -5  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can value converge still it has noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
