{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate policy using Monte-carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphics import *\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create 5x5 Grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = GraphWin(\"My Window\", 600,600)\n",
    "win.setBackground(color_rgb(255,255,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(x1,y1,x2,y2):\n",
    "    \"\"\"\n",
    "    create a line between pt(x1,y1) and pt(x2,y2) and return it\n",
    "    \"\"\"\n",
    "    ln = Line(Point(x1,y1),Point(x2,y2))\n",
    "    ln.setOutline(color_rgb(0,0,0))\n",
    "    ln.setWidth(2)\n",
    "    \n",
    "    return ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rectangle(Point(150.0, 250.0), Point(250.0, 350.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#border lines\n",
    "border1 = line(50,50,550,50)\n",
    "border1.draw(win)\n",
    "\n",
    "border2 = line(550,50,550,550)\n",
    "border2.draw(win)\n",
    "\n",
    "border3 = line(50,550,550,550)\n",
    "border3.draw(win)\n",
    "\n",
    "border4 = line(50,50,50,550)\n",
    "border4.draw(win)\n",
    "\n",
    "#lines horizontal\n",
    "ln1 = line(50,150,550,150)\n",
    "ln1.draw(win)\n",
    "\n",
    "ln2 = line(50,250,550,250)\n",
    "ln2.draw(win)\n",
    "\n",
    "ln3 = line(50,350,550,350)\n",
    "ln3.draw(win)\n",
    "\n",
    "ln4 = line(50,450,550,450)\n",
    "ln4.draw(win)\n",
    "\n",
    "#lines vertical\n",
    "\n",
    "lnv1 = line(150,50,150,550)\n",
    "lnv1.draw(win)\n",
    "\n",
    "lnv2 = line(250,50,250,550)\n",
    "lnv2.draw(win)\n",
    "\n",
    "lnv3 = line(350,50,350,550)\n",
    "lnv3.draw(win)\n",
    "\n",
    "lnv4 = line(450,50,450,550)\n",
    "lnv4.draw(win)\n",
    "\n",
    "#create goal, obstacle and hole\n",
    "\n",
    "rect_goal = Rectangle(Point(450,50), Point(550,150))\n",
    "rect_goal.setFill(color_rgb(0,255,0))\n",
    "rect_goal.draw(win)\n",
    "\n",
    "rect_hole = Rectangle(Point(450,150), Point(550,250))\n",
    "rect_hole.setFill(color_rgb(255,0,0))\n",
    "rect_hole.draw(win)\n",
    "\n",
    "rect_obs1 = Rectangle(Point(150,350), Point(250,450))\n",
    "rect_obs1.setFill(color_rgb(0,0,0))\n",
    "rect_obs1.draw(win)\n",
    "\n",
    "rect_obs2 = Rectangle(Point(150,250), Point(250,350))\n",
    "rect_obs2.setFill(color_rgb(0,0,0))\n",
    "rect_obs2.draw(win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create agent look like circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Circle(Point(100.0, 500.0), 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Circle(Point(100,500), 25)\n",
    "agent.setFill(color_rgb(0,0,255))\n",
    "agent.draw(win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if (val.getY()) == 100 or (val.getY() == 500 and val.getX() == 200) :\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(0,-100)\n",
    "        \n",
    "    \n",
    "def down(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getY() == 500 or (val.getX() == 200 and val.getY() == 200):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(0,100)\n",
    "\n",
    "def right(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getX() == 500 or (val.getX() == 100 and (val.getY() == 300 or val.getY() == 400)):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(100,0)\n",
    "\n",
    "def left(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getX() == 100 or (val.getX() == 300 or (val.getY() == 300 or val.getY() == 400)):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(-100,0)\n",
    "        \n",
    "def action(agent,val):\n",
    "    if val == 0:\n",
    "        up(agent)\n",
    "    elif val == 1:\n",
    "        down(agent)\n",
    "    elif val == 2:\n",
    "        right(agent)\n",
    "    else :\n",
    "        left(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward for agent in the each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.array([[-1,-1,-1,-1,-1],\n",
    "                   [-1,-20,-1,-1,-1],\n",
    "                   [-1,-20,-1,-1,-1],\n",
    "                   [-1,-1,-1,-1,-100],\n",
    "                   [-1,-1,-1,-1,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards(agent):\n",
    "    val = agent.getCenter()\n",
    "    i = int(val.getX() / 100) - 1\n",
    "    j = int((500 - val.getY()) / 100) \n",
    "    \n",
    "    \n",
    "    return reward[j][i]\n",
    "\n",
    "def observation(agent):\n",
    "    val = agent.getCenter()\n",
    "    i = int(val.getX() / 100) - 1\n",
    "    j = int((500 - val.getY()) / 100) \n",
    "    \n",
    "    return [i,j]\n",
    "\n",
    "def env_close():\n",
    "    win.close()\n",
    "\n",
    "def reset(agent):\n",
    "    agent.move(-400,400)\n",
    "\n",
    "def set_agent(agent,obs):\n",
    "    pos = agent.getCenter()\n",
    "    \n",
    "    pos = [pos.getX(), pos.getY()]\n",
    "    \n",
    "    refer_pos = [(obs[0]+1) * 100, (500 - (obs[1]*100))]\n",
    "    \n",
    "    \n",
    "    agent.move(refer_pos[0]-pos[0], refer_pos[1] - pos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the value function and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.array([[.0,.0,.0,.0,.0],\n",
    "                  [.0,.0,.0,.0,.0],\n",
    "                  [.0,.0,.0,.0,.0],\n",
    "                  [.0,.0,.0,.0,0.0],\n",
    "                  [.0,.0,.0,.0,100.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.array([[0,0,0,0,2],\n",
    "                   [2,0,0,0,2],\n",
    "                   [0,0,0,0,2],\n",
    "                   [0,0,0,0,2],\n",
    "                   [0,0,3,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_visit = np.zeros([5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 100\n",
    "max_episode_length = 100\n",
    "learning_rate = .01\n",
    "discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(episode):  \n",
    "    \n",
    "    Experience_replay = []\n",
    "    Return = 0\n",
    "    incremental_discount_prev = 1\n",
    "    incremental_discount_next = 1\n",
    "    \n",
    "    for i in range(max_episode_length):\n",
    "            \n",
    "        state = observation(agent)\n",
    "        \n",
    "        number_visit[state[0],state[1]] += 1\n",
    "        \n",
    "        if state[0]== 4 and state[1] == 4:\n",
    "            reset(agent)\n",
    "            break\n",
    "        \n",
    "        act = policy[state[0],state[1]]\n",
    "        \n",
    "        #add probability to each action\n",
    "        if act == 0:\n",
    "            act = np.random.choice([0,1,2,3],p=[0.8,0,0.1,0.1])\n",
    "        elif act == 1:\n",
    "            act = np.random.choice([0,1,2,3],p=[0,0.8,0.1,0.1])\n",
    "        elif act == 2:\n",
    "            act = np.random.choice([0,1,2,3],p=[0.1,0.1,0.8,0])\n",
    "        elif act == 3:\n",
    "            act = np.random.choice([0,1,2,3],p=[0.1,0.1,0, 0.8])\n",
    "            \n",
    "        #act on the grid world\n",
    "        action(agent,act)\n",
    "        \n",
    "        incremental_experience = [state[0],state[1],-Return,incremental_discount_next]\n",
    "        \n",
    "        Experience_replay.append(incremental_experience)\n",
    "        \n",
    "        incremental_discount_next = incremental_discount_next * discount_factor\n",
    "    \n",
    "        Reward = rewards(agent) * incremental_discount_prev\n",
    "        Return = Return + Reward\n",
    "        \n",
    "        incremental_discount_prev = incremental_discount_next\n",
    "        \n",
    "    \n",
    " \n",
    "    for exp in Experience_replay:\n",
    "        value[exp[0],exp[1]] = value[exp[0],exp[1]] + (learning_rate * (((Return+ exp[2])/ exp[3] )- value[exp[0],exp[1]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.52685631,  22.56335518,  26.97368345,  34.97266356,\n",
       "         36.25058242],\n",
       "       [  7.19104057,   0.        ,   0.        ,  14.41196371,\n",
       "         45.42710703],\n",
       "       [  7.817486  ,   8.44836567,   8.48207751,  24.05762917,\n",
       "         57.54884868],\n",
       "       [  0.62171   ,   2.00532509,   3.03040197,  14.90346537,\n",
       "         66.97957893],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "        100.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 4000 iteration value converge for the given policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 58 | 69 | 80 | 94 | 100 |\n",
    "|----|----|----|----|-----|\n",
    "| 51 | 59 | 69 | 72 | 57  |\n",
    "|----|----|----|----|-----|\n",
    "| 44 |  0 | 59 | 44 | -13 |\n",
    "|----|----|----|----|-----|\n",
    "| 37 |  0 | 51 | 28 | -5  |\n",
    "|----|----|----|----|-----|\n",
    "| 32 | 37 | 43 | 13 | -1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 2000 iteration value converge for the given policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 57 | 65 | 78 | 93 | 100 |\n",
    "|----|----|----|----|-----|\n",
    "| 50 | 57 | 70 | 70 | 37  |\n",
    "|----|----|----|----|-----|\n",
    "| 43 |  0 | 53 | 27 | -11 |\n",
    "|----|----|----|----|-----|\n",
    "| 37 |  0 | 47 | 15 | -2  |\n",
    "|----|----|----|----|-----|\n",
    "| 31 | 35 | 39 | 7  |-.5  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  After 100 iteration it evalute the given policy with this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 36 | 45 | 57 | 66 | 100|\n",
    "|----|----|----|----|----|\n",
    "| 34 | 14 | 24 | 14 |  0 |\n",
    "|----|----|----|----|----|\n",
    "| 27 |  0 |  8 | 3  |  0 |\n",
    "|----|----|----|----|----|\n",
    "| 22 |  0 |  8 | 2  |  0 |\n",
    "|----|----|----|----|----|\n",
    "| 22 |  7 | 8  | 1  |  0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see huge difference in convergene value compare between 100 and 2000 iteration and in the 2000 episode still some state visit only one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
