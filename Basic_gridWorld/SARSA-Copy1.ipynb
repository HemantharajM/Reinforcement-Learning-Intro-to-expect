{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphics import *\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create 5x5 Grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = GraphWin(\"My Window\", 600,600)\n",
    "win.setBackground(color_rgb(255,255,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(x1,y1,x2,y2):\n",
    "    \"\"\"\n",
    "    create a line between pt(x1,y1) and pt(x2,y2) and return it\n",
    "    \"\"\"\n",
    "    ln = Line(Point(x1,y1),Point(x2,y2))\n",
    "    ln.setOutline(color_rgb(0,0,0))\n",
    "    ln.setWidth(2)\n",
    "    \n",
    "    return ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rectangle(Point(150.0, 250.0), Point(250.0, 350.0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#border lines\n",
    "border1 = line(50,50,550,50)\n",
    "border1.draw(win)\n",
    "\n",
    "border2 = line(550,50,550,550)\n",
    "border2.draw(win)\n",
    "\n",
    "border3 = line(50,550,550,550)\n",
    "border3.draw(win)\n",
    "\n",
    "border4 = line(50,50,50,550)\n",
    "border4.draw(win)\n",
    "\n",
    "#lines horizontal\n",
    "ln1 = line(50,150,550,150)\n",
    "ln1.draw(win)\n",
    "\n",
    "ln2 = line(50,250,550,250)\n",
    "ln2.draw(win)\n",
    "\n",
    "ln3 = line(50,350,550,350)\n",
    "ln3.draw(win)\n",
    "\n",
    "ln4 = line(50,450,550,450)\n",
    "ln4.draw(win)\n",
    "\n",
    "#lines vertical\n",
    "\n",
    "lnv1 = line(150,50,150,550)\n",
    "lnv1.draw(win)\n",
    "\n",
    "lnv2 = line(250,50,250,550)\n",
    "lnv2.draw(win)\n",
    "\n",
    "lnv3 = line(350,50,350,550)\n",
    "lnv3.draw(win)\n",
    "\n",
    "lnv4 = line(450,50,450,550)\n",
    "lnv4.draw(win)\n",
    "\n",
    "#create goal, obstacle and hole\n",
    "\n",
    "rect_goal = Rectangle(Point(450,50), Point(550,150))\n",
    "rect_goal.setFill(color_rgb(0,255,0))\n",
    "rect_goal.draw(win)\n",
    "\n",
    "rect_hole = Rectangle(Point(450,150), Point(550,250))\n",
    "rect_hole.setFill(color_rgb(255,0,0))\n",
    "rect_hole.draw(win)\n",
    "\n",
    "rect_obs1 = Rectangle(Point(150,350), Point(250,450))\n",
    "rect_obs1.setFill(color_rgb(0,0,0))\n",
    "rect_obs1.draw(win)\n",
    "\n",
    "rect_obs2 = Rectangle(Point(150,250), Point(250,350))\n",
    "rect_obs2.setFill(color_rgb(0,0,0))\n",
    "rect_obs2.draw(win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create agent look like circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Circle(Point(100.0, 500.0), 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Circle(Point(100,500), 25)\n",
    "agent.setFill(color_rgb(0,0,255))\n",
    "agent.draw(win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if (val.getY()) == 100 or (val.getY() == 500 and val.getX() == 200) :\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(0,-100)\n",
    "        \n",
    "    \n",
    "def down(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getY() == 500 or (val.getX() == 200 and val.getY() == 200):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(0,100)\n",
    "\n",
    "def right(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getX() == 500 or (val.getX() == 100 and (val.getY() == 300 or val.getY() == 400)):\n",
    "        pass\n",
    "    else :\n",
    "        agent.move(100,0)\n",
    "\n",
    "def left(agent):\n",
    "    val = agent.getCenter()\n",
    "    \n",
    "    if val.getX() == 100 or (val.getX() == 300 and (val.getY() == 300 or val.getY() == 400)):\n",
    "        pass\n",
    "    else:\n",
    "        agent.move(-100,0)\n",
    "        \n",
    "def action(agent,val):\n",
    "    if val == 0:\n",
    "        up(agent)\n",
    "    elif val == 1:\n",
    "        down(agent)\n",
    "    elif val == 2:\n",
    "        right(agent)\n",
    "    else :\n",
    "        left(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(agent,act):\n",
    "    if act[0][0] > 0.1 and act[0][1] > 0.1:\n",
    "        action(agent,np.random.choice([0,2]))\n",
    "    elif act[0][0] > 0.1 and act[0][1] > -0.1 and act[0][1] < 0.1:\n",
    "        action(agent,2)\n",
    "    elif act[0][0] > 0.1 and act[0][1] < -0.1:\n",
    "        action(agent,np.random.choice([1,2]))\n",
    "    elif act[0][0] > -0.1 and act[0][0] < 0.1 and act[0][1] > 0.1:\n",
    "        action(agent,0)\n",
    "    elif act[0][0] > -0.1 and act[0][0] < 0.1 and act[0][1] > -0.1 and act[0][1] < 0.1:\n",
    "        action(agent,np.random.choice([0,1]))\n",
    "    elif act[0][0] > -0.1 and act[0][0] < 0.1 and act[0][1] < -0.1:\n",
    "        action(agent,1)\n",
    "    elif act[0][0] < -0.1 and act[0][1] > 0.1:\n",
    "        action(agent, np.random.choice([0,3]))\n",
    "    elif act[0][0] < -0.1 and act[0][1] > -0.1 and act[0][1] < 0.1:\n",
    "        action(agent, 3)\n",
    "    elif act[0][0] < -0.1 and act[0][1] < -0.1:\n",
    "        action(agent, np.random.choice([1,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward for agent in the each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.array([[-.1,-.1,-.1,-.1,-.1],\n",
    "                   [-.1,-.5,-.1,-.1,-.1],\n",
    "                   [-.1,-.5,-.1,-.1,-.1],\n",
    "                   [-.1,-.1,-.1,-.1,-1],\n",
    "                   [-.1,-.1,-.1,-.1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards(agent):\n",
    "    val = agent.getCenter()\n",
    "    i = int(val.getX() / 100) - 1\n",
    "    j = int((500 - val.getY()) / 100) \n",
    "    \n",
    "    \n",
    "    return reward[j][i]\n",
    "\n",
    "def observation(agent):\n",
    "    val = agent.getCenter()\n",
    "    i = int(val.getX() / 100) - 1\n",
    "    j = int((500 - val.getY()) / 100) \n",
    "    \n",
    "    return [i,j]\n",
    "\n",
    "def env_close():\n",
    "    win.close()\n",
    "\n",
    "def reset(agent):\n",
    "    agent.move(-400,400)\n",
    "\n",
    "def set_agent(agent,obs):\n",
    "    pos = agent.getCenter()\n",
    "    \n",
    "    pos = [pos.getX(), pos.getY()]\n",
    "    \n",
    "    refer_pos = [(obs[0]+1) * 100, (500 - (obs[1]*100))]\n",
    "    \n",
    "    \n",
    "    agent.move(refer_pos[0]-pos[0], refer_pos[1] - pos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the value function and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "\tdef __init__(self, sess):\n",
    "\t\tself.sess = sess\n",
    "\n",
    "\t\tself.learning_rate = 0.00006\n",
    "\t\tself.epsilon = .5\n",
    "\t\tself.epsilon_decay = 1.0\n",
    "\t\tself.gamma = .995\n",
    "\t\tself.tau   = .125\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                               Actor Model                             #\n",
    "\t\t# Chain rule: find the gradient of chaging the actor network params in  #\n",
    "\t\t# getting closest to the final value network predictions, i.e. de/dA    #\n",
    "\t\t# Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "\t\t# ===================================================================== #\n",
    "\n",
    "\t\tself.memory = deque(maxlen=2000)\n",
    "\t\tself.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "\t\t_, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "\t\tself.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "\t\t\t[None, 2]) # where we will feed de/dC (from critic)\n",
    "\t\t\n",
    "\t\tactor_model_weights = self.actor_model.trainable_weights\n",
    "\t\tself.actor_grads = tf.gradients(self.actor_model.output, \n",
    "\t\t\tactor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "\t\tgrads = zip(self.actor_grads, actor_model_weights)\n",
    "\t\tself.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "\t\t# ===================================================================== #\n",
    "\t\t#                              Critic Model                             #\n",
    "\t\t# ===================================================================== #\t\t\n",
    "\n",
    "\t\tself.critic_state_input, self.critic_action_input, \\\n",
    "\t\t\tself.critic_model = self.create_critic_model()\n",
    "\t\t_, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "\t\tself.critic_grads = tf.gradients(self.critic_model.output, \n",
    "\t\t\tself.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "\t\t\n",
    "\t\t# Initialize for later gradient calculations\n",
    "\t\tself.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Definitions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef create_actor_model(self):\n",
    "\t\tstate_input = Input(shape=(2,))\n",
    "\t\th1 = Dense(4, activation='relu') (state_input)\n",
    "\t\th2 = Dense(12, activation='relu')(h1)\n",
    "\t\th3 = Dense(12, activation='relu')(h2)\n",
    "\t\th4 = Dense( 6, activation='relu')(h3)\n",
    "\t\toutput = Dense(2, activation='linear')(h4)\n",
    "\t\t\n",
    "\t\tmodel = Model(input=state_input, output=output)\n",
    "\t\tadam  = Adam(lr=0.00003)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, model\n",
    "\n",
    "\tdef create_critic_model(self):\n",
    "\t\tstate_input = Input(shape=(2,))\n",
    "\t\tstate_h1 = Dense(4,activation='relu')(state_input)\n",
    "\t\tstate_h2 = Dense(12, activation='relu')(state_h1)\n",
    "\t\t\n",
    "\t\taction_input = Input(shape=(2,))\n",
    "\t\taction_h1 = Dense(4,activation='relu')(action_input)\n",
    "\t\taction_h2    = Dense(12)(action_h1)\n",
    "\t\t\n",
    "\t\tmerged    = Add()([state_h1, action_h1])\n",
    "\t\tmerged_h1 = Dense(24, activation='relu')(merged)\n",
    "\t\tmerged_h2 = Dense(12, activation='relu')(merged_h1)\n",
    "\t\tmerged_h3 = Dense(4,activation='relu')(merged_h2)\n",
    "\t\toutput = Dense(1, activation='linear')(merged_h3)\n",
    "\t\tmodel  = Model(input=[state_input,action_input], output=output)\n",
    "\t\t\n",
    "\t\tadam  = Adam(lr=0.00003)\n",
    "\t\tmodel.compile(loss=\"mse\", optimizer=adam)\n",
    "\t\treturn state_input, action_input, model\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                               Model Training                              #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef remember(self, cur_state, action, reward, new_state, done):\n",
    "\t\tself.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "\tdef _train_actor(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, _ = sample\n",
    "\t\t\tpredicted_action = self.actor_model.predict(cur_state)\n",
    "\t\t\tgrads = self.sess.run(self.critic_grads, feed_dict={\n",
    "\t\t\t\tself.critic_state_input:  cur_state,\n",
    "\t\t\t\tself.critic_action_input: predicted_action\n",
    "\t\t\t})[0]\n",
    "\n",
    "\t\t\tself.sess.run(self.optimize, feed_dict={\n",
    "\t\t\t\tself.actor_state_input: cur_state,\n",
    "\t\t\t\tself.actor_critic_grad: grads\n",
    "\t\t\t})\n",
    "            \n",
    "\tdef _train_critic(self, samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tcur_state, action, reward, new_state, done = sample\n",
    "\t\t\tif not done:\n",
    "\t\t\t\ttarget_action = self.target_actor_model.predict(new_state)\n",
    "\t\t\t\tfuture_reward = self.target_critic_model.predict(\n",
    "\t\t\t\t\t[new_state, target_action])[0][0]\n",
    "\t\t\t\treward += self.gamma * future_reward\n",
    "\t\t\tself.critic_model.fit([cur_state, action], np.array(reward).reshape(1,1),verbose= 0)\n",
    "\t\t\n",
    "\tdef train(self):\n",
    "\t\tbatch_size = 4\n",
    "\t\tif len(self.memory) < batch_size:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\trewards = []\n",
    "\t\tsamples = random.sample(self.memory, batch_size)\n",
    "\t\tself._train_critic(samples)\n",
    "\t\tself._train_actor(samples)\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                         Target Model Updating                             #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef _update_actor_target(self):\n",
    "\t\tactor_model_weights  = self.actor_model.get_weights()\n",
    "\t\tactor_target_weights = self.target_actor_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(actor_target_weights)):\n",
    "\t\t\tactor_target_weights[i] = actor_model_weights[i]\n",
    "\t\tself.target_actor_model.set_weights(actor_target_weights)\n",
    "\n",
    "\tdef _update_critic_target(self):\n",
    "\t\tcritic_model_weights  = self.critic_model.get_weights()\n",
    "\t\tcritic_target_weights = self.target_critic_model.get_weights()\n",
    "\t\t\n",
    "\t\tfor i in range(len(critic_target_weights)):\n",
    "\t\t\tcritic_target_weights[i] = critic_model_weights[i]\n",
    "\t\tself.target_critic_model.set_weights(critic_target_weights)\t\t\n",
    "\n",
    "\tdef update_target(self):\n",
    "\t\tself._update_actor_target()\n",
    "\t\tself._update_critic_target()\n",
    "\n",
    "\t# ========================================================================= #\n",
    "\t#                              Model Predictions                            #\n",
    "\t# ========================================================================= #\n",
    "\n",
    "\tdef act(self, cur_state):\n",
    "\t\tif np.random.random() < self.epsilon:\n",
    "\t\t\treturn np.array([np.random.uniform(-.5,.5), np.random.uniform(-.5,.5)])\n",
    "\t\treturn self.actor_model.predict(cur_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gideon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/home/gideon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "actor_critic = ActorCritic(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 5000\n",
    "max_episode_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[0.]]     [[-0.0601057]]     [[-0.12021139]]     [[-0.18031716]]     [[-0.24042279]]     \n",
      "[[0.1280388]]     [[0.00373794]]     [[-0.01393016]]     [[-0.07624605]]     [[-0.1407313]]     \n",
      "[[0.2560776]]     [[0.01843523]]     [[0.00747587]]     [[0.00897286]]     [[-0.02786032]]     \n",
      "[[0.38411644]]     [[0.1471787]]     [[0.00971683]]     [[0.01121381]]     [[0.01271079]]     \n",
      "[[0.5121552]]     [[0.26704192]]     [[0.03687047]]     [[0.01345476]]     [[0.01495175]]     \n",
      "25\n",
      "[[-0.01488974]]     [[0.06762932]]     [[0.15601096]]     [[0.16786297]]     [[0.13747337]]     \n",
      "[[0.03866294]]     [[-0.01488974]]     [[0.01625794]]     [[0.10434854]]     [[0.19123855]]     \n",
      "[[0.10298575]]     [[-0.00518486]]     [[-0.01488974]]     [[-0.01488974]]     [[0.05297877]]     \n",
      "[[0.16763411]]     [[0.05916696]]     [[-0.01450955]]     [[-0.01271214]]     [[-0.01091777]]     \n",
      "[[0.23403019]]     [[0.12348975]]     [[0.01528736]]     [[-0.01011688]]     [[-0.00832578]]     \n",
      "50\n",
      "[[-0.02178921]]     [[-0.01517007]]     [[0.04161277]]     [[0.08033173]]     [[0.09013697]]     \n",
      "[[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     \n",
      "[[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     \n",
      "[[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     \n",
      "[[-0.01056331]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     [[-0.02178921]]     \n",
      "75\n",
      "[[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.03529729]]     [[-0.00635586]]     \n",
      "[[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     \n",
      "[[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     \n",
      "[[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     \n",
      "[[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     [[-0.04111895]]     \n",
      "100\n",
      "[[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     \n",
      "[[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     \n",
      "[[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     \n",
      "[[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     \n",
      "[[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     [[-0.0621199]]     \n"
     ]
    }
   ],
   "source": [
    "for j in range(episode):  \n",
    "    \n",
    "    #update the target network\n",
    "    if ((j % 25) == 0):\n",
    "        print(j)\n",
    "        for k in range(5):\n",
    "            for l in range(5):\n",
    "                A = actor_critic.actor_model.predict(np.array([[k,l]]))\n",
    "                val = actor_critic.critic_model.predict([np.array([[k,l]]),A])\n",
    "        \n",
    "                print(val ,end = \"     \")\n",
    "            print()\n",
    "        \n",
    "        actor_critic.critic_model.save_weights('./critic')\n",
    "        actor_critic.actor_model.save_weights('./actor')\n",
    "        actor_critic.update_target()\n",
    "    \n",
    "    set_agent(agent,[0,0])\n",
    "    S = np.array(observation(agent))\n",
    "    S = S.reshape(1,2)\n",
    "    A = np.array([np.random.uniform(-5,5), np.random.uniform(-5,5)])\n",
    "    A = A.reshape(1,2)\n",
    "    \n",
    "    for i in range(max_episode_length):  \n",
    "        R = rewards(agent)\n",
    "        \n",
    "        A = actor_critic.act(S)\n",
    "        \n",
    "        A = A.reshape(1,2)\n",
    "        \n",
    "        step(agent,A)  \n",
    "        \n",
    "        S_new = np.array(observation(agent))\n",
    "        S_new = S_new.reshape(1,2)\n",
    "        \n",
    "        \n",
    "        if S_new[0][0] > -1 and S_new[0][1] > -1:\n",
    "            pass\n",
    "        else :\n",
    "            set_agent(agent,[0,0])\n",
    "            S_new[0][0] = 0\n",
    "            S_new[0][0] = 0\n",
    "        \n",
    "        if (S[0][0] == 4) and (S[0][1] == 4):\n",
    "            reset(agent)\n",
    "            done = True\n",
    "        else :\n",
    "            done = False\n",
    "        \n",
    "        actor_critic.remember(S,A,R,S_new,done)\n",
    "        \n",
    "        actor_critic.train()\n",
    "        \n",
    "        if done :\n",
    "            S = np.array(observation(agent))\n",
    "            S = S.reshape(1,2)\n",
    "            \n",
    "        S = S_new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 0]]),\n",
       " array([[-0.02112696, -0.27801025]], dtype=float32),\n",
       " -0.1,\n",
       " array([[0, 0]]),\n",
       " False]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_critic.memory[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(agent,np.array([[-0.0211,-0.278]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-48.73358]] 0 0     [[-48.35283]] 0 1     [[-48.23629]] 0 2     [[-48.18986]] 0 3     [[-48.120556]] 0 4     \n",
      "[[-48.925564]] 1 0     [[-49.430687]] 1 1     [[-48.160957]] 1 2     [[-48.117188]] 1 3     [[-48.067593]] 1 4     \n",
      "[[-48.55804]] 2 0     [[-49.586308]] 2 1     [[-48.988613]] 2 2     [[-48.029404]] 2 3     [[-47.98828]] 2 4     \n",
      "[[-50.596508]] 3 0     [[-49.730343]] 3 1     [[-49.869175]] 3 2     [[-48.510223]] 3 3     [[-47.89785]] 3 4     \n",
      "[[-53.613655]] 4 0     [[-52.427242]] 4 1     [[-51.804996]] 4 2     [[-50.085648]] 4 3     [[-48.031807]] 4 4     \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        A = actor_critic.actor_model.predict(np.array([[i,j]]))\n",
    "        val = actor_critic.critic_model.predict([np.array([[i,j]]),A])\n",
    "        \n",
    "        print(val,i,j ,end = \"     \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 300 iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| right| right| right| right|  GOAL  |\n",
    "|------|------|------|------|--------|\n",
    "| right| right|  up  |  up  |   up   |\n",
    "|------|------|------|------|--------|\n",
    "| left |      |  up  |  up  |  left  |\n",
    "|------|------|------|------|--------|\n",
    "| left |      |  up  |  up  | right  |\n",
    "|------|------|------|------|--------|\n",
    "| right| left |  up  | left |  left  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 1000 iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| right| right| right| right|  GOAL  |\n",
    "|------|------|------|------|--------|\n",
    "| right| right|  up  |  up  |   up   |\n",
    "|------|------|------|------|--------|\n",
    "|  up  |      |  up  |  up  |  left  |\n",
    "|------|------|------|------|--------|\n",
    "|  up  |      |  up  |  up  |  left  |\n",
    "|------|------|------|------|--------|\n",
    "|  up  | right|  up  | right|  right |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
